{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_task3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1e/tzq/+fnwM3QKKllAas",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ra-MANUJ-an/MIDAS-PROJECT/blob/main/Project_task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGmNTjuwhsIu"
      },
      "source": [
        "**IMPORTS**\n",
        "\n",
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy6usVcHX3s9",
        "outputId": "6443b562-8c75-4207-ef9e-d929bdfb9713"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile # extracting zip files\n",
        "import random # shuffling images\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)\n",
        "import gzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKJuKrRlh2FP"
      },
      "source": [
        "**RESNET UTILITIES**\n",
        "\n",
        "taken from : https://github.com/priya-dwivedi/Deep-Learning/blob/master/resnet_keras/resnets_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byTuRGCMYuFW"
      },
      "source": [
        "#resnets_utils.py\n",
        "\n",
        "def load_dataset(): \n",
        "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "    \n",
        "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "    \n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    m = X.shape[0]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[permutation,:,:,:]\n",
        "    shuffled_Y = Y[permutation,:]\n",
        "    \n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
        "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
        "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    return mini_batches\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y\n",
        "    \n",
        "def forward_propagation_for_predict(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3'] \n",
        "                                                           # Numpy Equivalents:\n",
        "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
        "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
        "    \n",
        "    return Z3\n",
        "    \n",
        "def predict(X, parameters):\n",
        "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
        "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
        "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
        "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
        "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
        "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
        "    \n",
        "    params = {\"W1\": W1,\n",
        "              \"b1\": b1,\n",
        "              \"W2\": W2,\n",
        "              \"b2\": b2,\n",
        "              \"W3\": W3,\n",
        "              \"b3\": b3}\n",
        "    \n",
        "    x = tf.placeholder(\"float\", [12288, 1])\n",
        "    \n",
        "    z3 = forward_propagation_for_predict(x, params)\n",
        "    p = tf.argmax(z3)\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    prediction = sess.run(p, feed_dict = {x: X})\n",
        "         \n",
        "    return prediction "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R468u2175m-"
      },
      "source": [
        "Extracting Files from \"trainPart1.zip\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf2Xcu169XHv",
        "outputId": "b59c4446-654e-49be-8ccb-9df7710f28cd"
      },
      "source": [
        "file_name = \"trainPart1.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip :\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvrXQ2psQXb-",
        "outputId": "1af3a0ef-0458-4a7c-b9ef-d26ce6eec713"
      },
      "source": [
        "file_name = \"mnistTask3.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip :\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-xNJKmf7tvn"
      },
      "source": [
        "Following function loads mnist images (both training and test) from .gz files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDUpq3TZQjLP"
      },
      "source": [
        "def load_data() :\n",
        "\n",
        "  def load_mnist_images(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "      data = data.reshape(-1,28,28,1)\n",
        "      \n",
        "      return data/np.float(256)\n",
        "\n",
        "  def load_mnist_labels(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "      \n",
        "      return data\n",
        "\n",
        "  X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "  y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "  X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "  y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "  # return X_train, y_train, X_test, y_test\n",
        "  return X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOz4fOtiQn_C"
      },
      "source": [
        "X_test, y_test = load_data() # loads mnist test data, as we need only test dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrnyIcawSGYP",
        "outputId": "63d1581e-f26b-4473-ff05-3d86e018fda8"
      },
      "source": [
        "DATADIRECTORY = \"/content/mnistTask\" # for extracting train mnist dataset from given dataset\n",
        "CATEGORIES = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
        "IMG_SIZE = 28\n",
        "\n",
        "trainingData = []\n",
        "\n",
        "def createTrainingData():\n",
        "    for category in CATEGORIES:\n",
        "\n",
        "        path = os.path.join(DATADIRECTORY,category)\n",
        "        classNum = CATEGORIES.index(category)  # get the classification\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                imgArray = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "                newArray = cv2.resize(imgArray, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                trainingData.append([newArray, classNum])  # add this to our trainingData\n",
        "            except Exception as e:  # keeps output cleans\n",
        "                pass\n",
        "\n",
        "createTrainingData()\n",
        "print(len(trainingData))\n",
        "\n",
        "random.shuffle(trainingData)\n",
        "# we are shuffling the data so that classifier does not \n",
        "# look at same type of images for a long time\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for features, label in trainingData :\n",
        "  X_train.append(features)\n",
        "  y_train.append(label)\n",
        "\n",
        "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5980/5980 [00:00<00:00, 21740.21it/s]\n",
            "100%|██████████| 5807/5807 [00:00<00:00, 19822.07it/s]\n",
            "100%|██████████| 6009/6009 [00:00<00:00, 20718.99it/s]\n",
            "100%|██████████| 6037/6037 [00:00<00:00, 20520.54it/s]\n",
            "100%|██████████| 5914/5914 [00:00<00:00, 21241.27it/s]\n",
            "100%|██████████| 6139/6139 [00:00<00:00, 21538.65it/s]\n",
            "100%|██████████| 6037/6037 [00:00<00:00, 21282.76it/s]\n",
            "100%|██████████| 5954/5954 [00:00<00:00, 21859.43it/s]\n",
            "100%|██████████| 6129/6129 [00:00<00:00, 19644.53it/s]\n",
            "100%|██████████| 5994/5994 [00:00<00:00, 21422.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHextMC9BqG"
      },
      "source": [
        "DATADIRECTORY1 = \"/content/train\"\n",
        "CATEGORIES = [\"Sample001\", \"Sample002\", \"Sample003\", \"Sample004\", \"Sample005\", \"Sample006\", \"Sample007\", \"Sample008\", \"Sample009\", \"Sample010\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi9mPzuTYEyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0011ee70-a246-40f0-febc-38b9a96f0c32"
      },
      "source": [
        "classSize = len(CATEGORIES)\n",
        "print(classSize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb2kBuOIJimp"
      },
      "source": [
        "IMG_SIZE = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnOaPz7bJoFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c17bb6d-2b20-401b-e734-9be14bf298cc"
      },
      "source": [
        "trainingData1 = []\n",
        "\n",
        "def createTrainingData1():\n",
        "    for category in CATEGORIES:\n",
        "\n",
        "        path = os.path.join(DATADIRECTORY1,category)\n",
        "        classNum1 = CATEGORIES.index(category)  # get the classification\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                imgArray1 = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "                newArray1 = cv2.resize(imgArray1, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                trainingData1.append([newArray1, classNum1])  # add this to our trainingData1\n",
        "            except Exception as e:  # keeps output cleans\n",
        "                pass\n",
        "\n",
        "createTrainingData1()\n",
        "print(len(trainingData1))\n",
        "\n",
        "random.shuffle(trainingData1)\n",
        "# we are shuffling the data so that classifier does not \n",
        "# look at same type of images for a long time\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for features, label in trainingData1 :\n",
        "  X.append(features)\n",
        "  y.append(label)\n",
        "\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:00<00:00, 104.98it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 114.14it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 110.44it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 108.93it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 114.61it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 109.43it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 110.04it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 113.30it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 112.93it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 105.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOeQWnfkiJ0L"
      },
      "source": [
        "**SMALL RESIDUAL NETWORK**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APCSUjCSR-qA"
      },
      "source": [
        "# Normalize image vectors\n",
        "X = X/255.0\n",
        "\n",
        "model = Sequential()\n",
        "# conv layer\n",
        "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # converts our 3D feature vectors to 1D feature vectors\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(classSize))  # our output layer. \n",
        "# Softmax for probability distribution\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # what to track"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTyCtbovIaz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35d9897-a33a-4290-e463-9b909359c04f"
      },
      "source": [
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y = np.array(y)\n",
        "model.fit(X, y, batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "13/13 [==============================] - 1s 8ms/step - loss: 2.3317 - accuracy: 0.1073\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 2.3053 - accuracy: 0.0913\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 2.3023 - accuracy: 0.1041\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 2.2979 - accuracy: 0.1211\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 2.3004 - accuracy: 0.1156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb9fc1e7390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Z4aWMYQvIl",
        "outputId": "f49ff72b-50a7-4be0-910e-860e12925a30"
      },
      "source": [
        "# Normalize image vectors\n",
        "X_train = X_train/255.\n",
        "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 2.3031 - accuracy: 0.1000\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 2.3028 - accuracy: 0.1006\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 2.3027 - accuracy: 0.1015\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 2.3027 - accuracy: 0.0993\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 2.3026 - accuracy: 0.1008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbacc76a690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tburI58XQ2Ba",
        "outputId": "f5abbcf4-ca59-4643-a4de-4fc76b28a904"
      },
      "source": [
        "X_test = X_test/255.\n",
        "X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_test = np.array(y_test)\n",
        "predictions = model.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3040 - accuracy: 0.0892\n",
            "Loss = 2.304044008255005\n",
            "Test Accuracy = 0.08919999748468399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7M4bu4qIYzF"
      },
      "source": [
        "Above model give results for pretrained model on trainPart1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlHWqY9THcWN"
      },
      "source": [
        "For randomly initialized weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA9g3aw8HYya"
      },
      "source": [
        "\n",
        "model1 = Sequential()\n",
        "# conv layer\n",
        "model1.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model1.add(Conv2D(256, (3, 3)))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model1.add(Flatten())  # converts our 3D feature vectors to 1D feature vectors\n",
        "\n",
        "model1.add(Dense(64))\n",
        "model1.add(Activation('relu'))\n",
        "\n",
        "model1.add(Dense(128))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "\n",
        "model1.add(Dense(128))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "\n",
        "model1.add(Dense(classSize))  # our output layer. \n",
        "# Softmax for probability distribution\n",
        "model1.add(Activation('softmax'))\n",
        "\n",
        "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # what to track"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsSYpPeVHo-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b0d4484-baba-4d04-8d4e-9df335e92bf3"
      },
      "source": [
        "model1.fit(X_train, y_train, batch_size=32, epochs=5)\n",
        "predictions = model1.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 5ms/step - loss: 2.3041 - accuracy: 0.0988\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 2.3028 - accuracy: 0.0998\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 2.3026 - accuracy: 0.1021\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 2.3024 - accuracy: 0.1043\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 2.3026 - accuracy: 0.1005\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 2.3035 - accuracy: 0.0974\n",
            "Loss = 2.303539991378784\n",
            "Test Accuracy = 0.09740000218153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhl0F9bTgb3I"
      },
      "source": [
        "**RESNET50**\n",
        "\n",
        "Implementation inspiration from\n",
        "- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - [Deep Residual Learning for Image Recognition (2015)](https://arxiv.org/abs/1512.03385)\n",
        "- Francois Chollet's GitHub repository: https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKy4KiJ2gfXG"
      },
      "source": [
        "# GRADED FUNCTION: identity_block\n",
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # Skips over 3 hidden layers\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "        \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X,X_shortcut])\n",
        "    # pass it through a RELU activation\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z-aHIVMhz38"
      },
      "source": [
        "# GRADED FUNCTION: convolutional_block\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Extract Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value as shortcut\n",
        "    X_shortcut = X\n",
        "\n",
        "    # MAIN PATH #\n",
        "\n",
        "    # First component of main path \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(F2, (f, f), strides = (1,1), name = conv_name_base + '2b', padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # SHORTCUT PATH #\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X, X_shortcut])\n",
        "    # pass through RELU activation\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeZGeuXbh6yX"
      },
      "source": [
        "# GRADED FUNCTION: ResNet50\n",
        "# returns resnet model\n",
        "def ResNet50(input_shape, classes):\n",
        "    \"\"\"\n",
        "    Implementation of the popular ResNet50 the following architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    # Zero-Padding, pads the input with pad(3,3)\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = AveragePooling2D((2, 2), padding='same')(X)\n",
        "    \n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbvrjB8aiR2F"
      },
      "source": [
        "model = ResNet50(input_shape = (IMG_SIZE, IMG_SIZE, 1), classes = classSize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzYSxKstijwg"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi04Iu8wimtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a478db0-f6ca-4d12-fead-0f026736227c"
      },
      "source": [
        "# Normalize image vectors\n",
        "# X = X/255.\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "y = convert_to_one_hot(y, classSize).T\n",
        "\n",
        "print (\"number of training examples = \" + str(X.shape[0]))\n",
        "print (\"X shape: \" + str(X.shape))\n",
        "print (\"y shape: \" + str(y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 400\n",
            "X shape: (400, 28, 28, 1)\n",
            "y shape: (400, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TjULdxKjEQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83bdae6b-5394-45ec-e1de-02efbcd1b274"
      },
      "source": [
        "# model.fit(X, y, epochs = 6, batch_size = 32)\n",
        "model.fit(X, y, batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "13/13 [==============================] - 5s 44ms/step - loss: 4.7541 - accuracy: 0.1480\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 37ms/step - loss: 3.1867 - accuracy: 0.2531\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 37ms/step - loss: 2.1909 - accuracy: 0.3686\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 38ms/step - loss: 1.6364 - accuracy: 0.4659\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 37ms/step - loss: 1.2971 - accuracy: 0.6443\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb9fc43ed90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv5Nia3G5eTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81e28204-3afb-494c-cf8e-2a245ead2855"
      },
      "source": [
        "# Normalize image vectors\n",
        "X_train = X_train/255.\n",
        "X_test = X_test/255.\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "y_train = convert_to_one_hot(y_train, classSize).T\n",
        "y_test = convert_to_one_hot(y_test, classSize).T\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"y_train shape: \" + str(y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"y_test shape: \" + str(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 60000\n",
            "number of test examples = 10000\n",
            "X_train shape: (60000, 28, 28, 1)\n",
            "y_train shape: (60000, 10)\n",
            "X_test shape: (10000, 28, 28, 1)\n",
            "y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK4zqSFJ6RKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614e0627-3e63-4a3c-cb51-9edf5ccf0b1f"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 73s 38ms/step - loss: 2.7665 - accuracy: 0.1022\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 70s 37ms/step - loss: 2.6514 - accuracy: 0.1001\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 70s 38ms/step - loss: 2.5728 - accuracy: 0.1000\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 70s 38ms/step - loss: 2.5294 - accuracy: 0.1027\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 70s 38ms/step - loss: 2.4608 - accuracy: 0.1033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb9e4080250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LJ8NiF26jw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1805b2f4-151b-482b-b538-9f4cdc84878f"
      },
      "source": [
        "predictions = model.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 5s 13ms/step - loss: 2.3053 - accuracy: 0.1032\n",
            "Loss = 2.3053061962127686\n",
            "Test Accuracy = 0.10320000350475311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Xkh8vWhdER"
      },
      "source": [
        "Sequential model and resnet50 model give similar results on this new mnist dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6utFR6E9i4LQ"
      },
      "source": [
        "Now let us train randomly initialized network (without training on trainPart1.zip files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoDfLlESh50_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db9971e5-45b3-4039-988d-86ccbec14c61"
      },
      "source": [
        "model = ResNet50(input_shape = (IMG_SIZE, IMG_SIZE, 1), classes = classSize)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=5)\n",
        "\n",
        "predictions = model.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 74s 37ms/step - loss: 2.9408 - accuracy: 0.1032\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 70s 37ms/step - loss: 2.6981 - accuracy: 0.0995\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 70s 37ms/step - loss: 2.5819 - accuracy: 0.0997\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 70s 37ms/step - loss: 2.5715 - accuracy: 0.1028\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 70s 37ms/step - loss: 2.5243 - accuracy: 0.1004\n",
            "313/313 [==============================] - 5s 12ms/step - loss: 2.3085 - accuracy: 0.0974\n",
            "Loss = 2.3085269927978516\n",
            "Test Accuracy = 0.09740000218153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnmuUGCLBcQe"
      },
      "source": [
        "Difference I can see in both of the datasets is even though mnist dataset from yann le cun website did not give test accuracy as high compared to train accuracy (in case of resnet model) but it was higher compared to test accuracy given by this dataset (new mnist).\n",
        "It is infact guessing the number with probability ~ 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7uEhl_QjSyn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}