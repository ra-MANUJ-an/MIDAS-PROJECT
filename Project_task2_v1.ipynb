{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_task2_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNH62Qneso3amfymMaOfsDA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ra-MANUJ-an/MIDAS-PROJECT/blob/main/Project_task2_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGmNTjuwhsIu"
      },
      "source": [
        "**IMPORTS**\n",
        "\n",
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy6usVcHX3s9",
        "outputId": "59fe7b89-7bac-4025-e213-2433983c5a55"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile # extracting zip files\n",
        "import random # shuffling images\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)\n",
        "import gzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKJuKrRlh2FP"
      },
      "source": [
        "**RESNET UTILITIES**\n",
        "\n",
        "taken from : https://github.com/priya-dwivedi/Deep-Learning/blob/master/resnet_keras/resnets_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byTuRGCMYuFW"
      },
      "source": [
        "#resnets_utils.py\n",
        "\n",
        "def load_dataset(): \n",
        "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "    \n",
        "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "    \n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    m = X.shape[0]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[permutation,:,:,:]\n",
        "    shuffled_Y = Y[permutation,:]\n",
        "    \n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
        "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
        "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    return mini_batches\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y\n",
        "    \n",
        "def forward_propagation_for_predict(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3'] \n",
        "                                                           # Numpy Equivalents:\n",
        "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
        "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
        "    \n",
        "    return Z3\n",
        "    \n",
        "def predict(X, parameters):\n",
        "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
        "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
        "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
        "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
        "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
        "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
        "    \n",
        "    params = {\"W1\": W1,\n",
        "              \"b1\": b1,\n",
        "              \"W2\": W2,\n",
        "              \"b2\": b2,\n",
        "              \"W3\": W3,\n",
        "              \"b3\": b3}\n",
        "    \n",
        "    x = tf.placeholder(\"float\", [12288, 1])\n",
        "    \n",
        "    z3 = forward_propagation_for_predict(x, params)\n",
        "    p = tf.argmax(z3)\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    prediction = sess.run(p, feed_dict = {x: X})\n",
        "         \n",
        "    return prediction "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R468u2175m-"
      },
      "source": [
        "Extracting Files from \"trainPart1.zip\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf2Xcu169XHv",
        "outputId": "35ce50e2-7566-4b7e-c0bb-3d7863f0526e"
      },
      "source": [
        "file_name = \"trainPart1.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip :\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHextMC9BqG"
      },
      "source": [
        "DATADIRECTORY = \"/content/train\"\n",
        "CATEGORIES = [\"Sample001\", \"Sample002\", \"Sample003\", \"Sample004\", \"Sample005\", \"Sample006\", \"Sample007\", \"Sample008\", \"Sample009\", \"Sample010\",\n",
        " \"Sample011\", \"Sample012\", \"Sample013\", \"Sample014\", \"Sample015\", \"Sample016\", \"Sample017\", \"Sample018\", \"Sample019\", \"Sample020\",\n",
        " \"Sample021\", \"Sample022\", \"Sample023\", \"Sample024\", \"Sample025\", \"Sample036\", \"Sample027\", \"Sample028\", \"Sample029\", \"Sample030\",\n",
        " \"Sample031\", \"Sample032\", \"Sample033\", \"Sample034\", \"Sample035\", \"Sample036\", \"Sample037\", \"Sample038\", \"Sample039\", \"Sample040\",\n",
        " \"Sample041\", \"Sample042\", \"Sample043\", \"Sample044\", \"Sample045\", \"Sample046\", \"Sample047\", \"Sample048\", \"Sample049\", \"Sample050\",\n",
        " \"Sample051\", \"Sample052\", \"Sample053\", \"Sample054\", \"Sample055\", \"Sample056\", \"Sample057\", \"Sample058\", \"Sample059\", \"Sample060\",\n",
        " \"Sample061\", \"Sample062\"]\n",
        "CATEGORIES = [\"Sample001\", \"Sample002\", \"Sample003\", \"Sample004\", \"Sample005\", \"Sample006\", \"Sample007\", \"Sample008\", \"Sample009\", \"Sample010\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi9mPzuTYEyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9a6d287-e4d3-4c1c-9fa9-145704c4a257"
      },
      "source": [
        "classSize = len(CATEGORIES)\n",
        "print(classSize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb2kBuOIJimp"
      },
      "source": [
        "IMG_SIZE = 28\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnOaPz7bJoFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4acc456-c1f8-47e4-ff5d-192f18051e23"
      },
      "source": [
        "trainingData = []\n",
        "\n",
        "def createTrainingData():\n",
        "    for category in CATEGORIES:\n",
        "\n",
        "        path = os.path.join(DATADIRECTORY,category)\n",
        "        classNum = CATEGORIES.index(category)  # get the classification\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                imgArray = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "                newArray = cv2.resize(imgArray, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                trainingData.append([newArray, classNum])  # add this to our trainingData\n",
        "            except Exception as e:  # keeps output cleans\n",
        "                pass\n",
        "\n",
        "createTrainingData()\n",
        "print(len(trainingData))\n",
        "\n",
        "random.shuffle(trainingData)\n",
        "# we are shuffling the data so that classifier does not \n",
        "# look at same type of images for a long time\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for features, label in trainingData :\n",
        "  X.append(features)\n",
        "  y.append(label)\n",
        "\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:00<00:00, 86.56it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 87.89it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 87.02it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 84.52it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 88.43it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 86.14it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 85.47it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 89.70it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 88.91it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 88.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOeQWnfkiJ0L"
      },
      "source": [
        "**SMALL RESIDUAL NETWORK**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APCSUjCSR-qA"
      },
      "source": [
        "# Normalize image vectors\n",
        "X = X/255.0\n",
        "\n",
        "model = Sequential()\n",
        "# conv layer\n",
        "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # converts our 3D feature vectors to 1D feature vectors\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(classSize))  # our output layer. \n",
        "# Softmax for probability distribution\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # what to track"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTyCtbovIaz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e875e620-1652-4ecf-d25d-8731edd123d9"
      },
      "source": [
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y = np.array(y)\n",
        "model.fit(X, y, batch_size=32, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "13/13 [==============================] - 1s 17ms/step - loss: 2.3429 - accuracy: 0.1137\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 2.3055 - accuracy: 0.1039\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 2.3084 - accuracy: 0.0884\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 2.3008 - accuracy: 0.0671\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 2.2937 - accuracy: 0.1266\n",
            "Epoch 6/20\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 2.2804 - accuracy: 0.1618\n",
            "Epoch 7/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 2.2676 - accuracy: 0.1557\n",
            "Epoch 8/20\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 2.1586 - accuracy: 0.2127\n",
            "Epoch 9/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 1.9603 - accuracy: 0.2676\n",
            "Epoch 10/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 1.8346 - accuracy: 0.3260\n",
            "Epoch 11/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 1.5879 - accuracy: 0.4166\n",
            "Epoch 12/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 1.4740 - accuracy: 0.4971\n",
            "Epoch 13/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 1.1629 - accuracy: 0.6212\n",
            "Epoch 14/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 1.0252 - accuracy: 0.6687\n",
            "Epoch 15/20\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.9029 - accuracy: 0.7305\n",
            "Epoch 16/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 0.7029 - accuracy: 0.7679\n",
            "Epoch 17/20\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.6871 - accuracy: 0.7615\n",
            "Epoch 18/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 0.5121 - accuracy: 0.8297\n",
            "Epoch 19/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 0.4531 - accuracy: 0.8397\n",
            "Epoch 20/20\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 0.3561 - accuracy: 0.8606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0998f5f790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-xNJKmf7tvn"
      },
      "source": [
        "Following function loads mnist images (both training and test) from .gz files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDUpq3TZQjLP"
      },
      "source": [
        "def load_data() :\n",
        "\n",
        "  def load_mnist_images(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "      data = data.reshape(-1,28,28,1)\n",
        "      \n",
        "      return data/np.float(256)\n",
        "\n",
        "  def load_mnist_labels(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "      \n",
        "      return data\n",
        "\n",
        "  X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "  y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "  X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "  y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOz4fOtiQn_C"
      },
      "source": [
        "X_train, y_train, X_test, y_test = load_data() # loads mnist data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Z4aWMYQvIl",
        "outputId": "3a98064a-13f0-4ff2-a15a-43dd8dd5c853"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1801 - accuracy: 0.9503\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0735 - accuracy: 0.9817\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0606 - accuracy: 0.9850\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0478 - accuracy: 0.9890\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0410 - accuracy: 0.9903\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0365 - accuracy: 0.9913\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0307 - accuracy: 0.9925\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0261 - accuracy: 0.9929\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0258 - accuracy: 0.9941\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0236 - accuracy: 0.9946\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0191 - accuracy: 0.9953\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0174 - accuracy: 0.9959\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0182 - accuracy: 0.9955\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0210 - accuracy: 0.9952\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0168 - accuracy: 0.9960\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0126 - accuracy: 0.9970\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0167 - accuracy: 0.9959\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0125 - accuracy: 0.9972\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0149 - accuracy: 0.9968\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0177 - accuracy: 0.9962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f099b0431d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tburI58XQ2Ba",
        "outputId": "51fedca3-916e-45f2-c287-6a86d5931bc7"
      },
      "source": [
        "predictions = model.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0702 - accuracy: 0.9911\n",
            "Loss = 0.07015903294086456\n",
            "Test Accuracy = 0.991100013256073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLfV_yk4TEFR"
      },
      "source": [
        "Test accuracy is pretty good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhl0F9bTgb3I"
      },
      "source": [
        "**RESNET50**\n",
        "\n",
        "Implementation inspiration from\n",
        "- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - [Deep Residual Learning for Image Recognition (2015)](https://arxiv.org/abs/1512.03385)\n",
        "- Francois Chollet's GitHub repository: https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKy4KiJ2gfXG"
      },
      "source": [
        "# GRADED FUNCTION: identity_block\n",
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # Skips over 3 hidden layers\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "        \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X,X_shortcut])\n",
        "    # pass it through a RELU activation\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z-aHIVMhz38"
      },
      "source": [
        "# GRADED FUNCTION: convolutional_block\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Extract Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value as shortcut\n",
        "    X_shortcut = X\n",
        "\n",
        "    # MAIN PATH #\n",
        "\n",
        "    # First component of main path \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(F2, (f, f), strides = (1,1), name = conv_name_base + '2b', padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # SHORTCUT PATH #\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X, X_shortcut])\n",
        "    # pass through RELU activation\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeZGeuXbh6yX"
      },
      "source": [
        "# GRADED FUNCTION: ResNet50\n",
        "# returns resnet model\n",
        "def ResNet50(input_shape, classes):\n",
        "    \"\"\"\n",
        "    Implementation of the popular ResNet50 the following architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    # Zero-Padding, pads the input with pad(3,3)\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = AveragePooling2D((2, 2), padding='same')(X)\n",
        "    \n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbvrjB8aiR2F"
      },
      "source": [
        "model = ResNet50(input_shape = (IMG_SIZE, IMG_SIZE, 1), classes = classSize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzYSxKstijwg"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi04Iu8wimtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c697f7a-ca4a-4240-be2c-b2b7f807207c"
      },
      "source": [
        "# Normalize image vectors\n",
        "X = X/255.\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "y = convert_to_one_hot(y, classSize).T\n",
        "\n",
        "print (\"number of training examples = \" + str(X.shape[0]))\n",
        "print (\"X shape: \" + str(X.shape))\n",
        "print (\"y shape: \" + str(y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 400\n",
            "X shape: (400, 28, 28, 1)\n",
            "y shape: (400, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TjULdxKjEQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab66e00-6581-45a2-f3d8-090f71eecd52"
      },
      "source": [
        "# model.fit(X, y, epochs = 6, batch_size = 32)\n",
        "model.fit(X, y, batch_size=32, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "13/13 [==============================] - 9s 88ms/step - loss: 4.8358 - accuracy: 0.1394\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 3.3307 - accuracy: 0.2705\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 1.8984 - accuracy: 0.3283\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 1.6498 - accuracy: 0.5209\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 1.2974 - accuracy: 0.6672\n",
            "Epoch 6/20\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.6559 - accuracy: 0.8183\n",
            "Epoch 7/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.5449 - accuracy: 0.8606\n",
            "Epoch 8/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.5777 - accuracy: 0.8293\n",
            "Epoch 9/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.5986 - accuracy: 0.8088\n",
            "Epoch 10/20\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 0.5680 - accuracy: 0.7809\n",
            "Epoch 11/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.3218 - accuracy: 0.9110\n",
            "Epoch 12/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.3980 - accuracy: 0.9111\n",
            "Epoch 13/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.2910 - accuracy: 0.9228\n",
            "Epoch 14/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.1953 - accuracy: 0.9359\n",
            "Epoch 15/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.1990 - accuracy: 0.9492\n",
            "Epoch 16/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.6562 - accuracy: 0.9138\n",
            "Epoch 17/20\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 0.8347 - accuracy: 0.7914\n",
            "Epoch 18/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.7405 - accuracy: 0.7349\n",
            "Epoch 19/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.6943 - accuracy: 0.8799\n",
            "Epoch 20/20\n",
            "13/13 [==============================] - 1s 88ms/step - loss: 0.2562 - accuracy: 0.9161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f09971c8f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl-7yJBfjLdc"
      },
      "source": [
        "def load_data() :\n",
        "\n",
        "  def load_mnist_images(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "      data = data.reshape(-1,28,28,1)\n",
        "      \n",
        "      return data/np.float(256)\n",
        "\n",
        "  def load_mnist_labels(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "      \n",
        "      return data\n",
        "\n",
        "  X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "  y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "  X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "  y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPQXqxlE5bMc"
      },
      "source": [
        "X_train, y_train, X_test, y_test = load_data() # loads mnist data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv5Nia3G5eTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f53707-46cf-46d5-ef7d-78b4ff2d9cd7"
      },
      "source": [
        "# Normalize image vectors\n",
        "X_train = X_train/255.\n",
        "X_test = X_test/255.\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "y_train = convert_to_one_hot(y_train, classSize).T\n",
        "y_test = convert_to_one_hot(y_test, classSize).T\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"y_train shape: \" + str(y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"y_test shape: \" + str(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 60000\n",
            "number of test examples = 10000\n",
            "X_train shape: (60000, 28, 28, 1)\n",
            "y_train shape: (60000, 10)\n",
            "X_test shape: (10000, 28, 28, 1)\n",
            "y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK4zqSFJ6RKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40280e24-cdb1-4761-d44a-ceb0a92a96a3"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 174s 91ms/step - loss: 0.2649 - accuracy: 0.9335\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 169s 90ms/step - loss: 0.2764 - accuracy: 0.9383\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.3030 - accuracy: 0.9290\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 170s 91ms/step - loss: 0.1515 - accuracy: 0.9607\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 169s 90ms/step - loss: 0.1494 - accuracy: 0.9658\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.0787 - accuracy: 0.9782\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.1378 - accuracy: 0.9699\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.1067 - accuracy: 0.9723\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.0725 - accuracy: 0.9824\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 170s 91ms/step - loss: 0.0751 - accuracy: 0.9812\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 170s 91ms/step - loss: 0.0598 - accuracy: 0.9842\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.0407 - accuracy: 0.9892\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.0541 - accuracy: 0.9863\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 170s 91ms/step - loss: 0.0327 - accuracy: 0.9905\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.0291 - accuracy: 0.9908\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 170s 91ms/step - loss: 0.0377 - accuracy: 0.9904\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 170s 91ms/step - loss: 0.0326 - accuracy: 0.9903\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.0252 - accuracy: 0.9926\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 171s 91ms/step - loss: 0.0213 - accuracy: 0.9940\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 170s 91ms/step - loss: 0.0371 - accuracy: 0.9922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0995fdf990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LJ8NiF26jw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2baf3f98-472e-43d8-cc40-a0f91d10c1ef"
      },
      "source": [
        "predictions = model.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 7s 24ms/step - loss: 1.6648 - accuracy: 0.5227\n",
            "Loss = 1.664808988571167\n",
            "Test Accuracy = 0.5227000117301941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z34-UgEMxj3M"
      },
      "source": [
        "Resnet model has a variance problem and is overfitting the training set with large number of iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Xkh8vWhdER"
      },
      "source": [
        "Sequential model outperformed resnet50 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6utFR6E9i4LQ"
      },
      "source": [
        "Now let us train randomly initialized network (without training on trainPart1.zip files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoDfLlESh50_",
        "outputId": "233badd7-1a28-4e73-95be-f717814a5a91"
      },
      "source": [
        "model = ResNet50(input_shape = (IMG_SIZE, IMG_SIZE, 1), classes = classSize)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=20)\n",
        "\n",
        "predictions = model.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 172s 89ms/step - loss: 0.6967 - accuracy: 0.8340\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 167s 89ms/step - loss: 0.3226 - accuracy: 0.9268\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 167s 89ms/step - loss: 0.2775 - accuracy: 0.9433\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 167s 89ms/step - loss: 0.1186 - accuracy: 0.9686\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 167s 89ms/step - loss: 0.1973 - accuracy: 0.9562\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 167s 89ms/step - loss: 0.1469 - accuracy: 0.9636\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 167s 89ms/step - loss: 0.1307 - accuracy: 0.9699\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 168s 89ms/step - loss: 0.0764 - accuracy: 0.9812\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.0455 - accuracy: 0.9863\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 168s 89ms/step - loss: 0.0760 - accuracy: 0.9815\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.0661 - accuracy: 0.9839\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.0495 - accuracy: 0.9878\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.0315 - accuracy: 0.9906\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.0342 - accuracy: 0.9901\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 167s 89ms/step - loss: 0.0719 - accuracy: 0.9862\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 167s 89ms/step - loss: 0.0402 - accuracy: 0.9892\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.0308 - accuracy: 0.9908\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.0374 - accuracy: 0.9904\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.0185 - accuracy: 0.9944\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.0206 - accuracy: 0.9939\n",
            "313/313 [==============================] - 8s 23ms/step - loss: 0.9250 - accuracy: 0.6930\n",
            "Loss = 0.9249952435493469\n",
            "Test Accuracy = 0.6930000185966492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7uEhl_QjSyn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}