{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_task2_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyyX7eVxjW/It91B5l45Zn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ra-MANUJ-an/MIDAS-PROJECT/blob/main/Project_task2_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGmNTjuwhsIu"
      },
      "source": [
        "**IMPORTS**\n",
        "\n",
        "Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy6usVcHX3s9",
        "outputId": "c3469e63-117f-43ce-987d-5190db894960"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from zipfile import ZipFile # extracting zip files\n",
        "import random # shuffling images\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "import h5py\n",
        "import math\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)\n",
        "import gzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKJuKrRlh2FP"
      },
      "source": [
        "**RESNET UTILITIES**\n",
        "\n",
        "taken from : https://github.com/priya-dwivedi/Deep-Learning/blob/master/resnet_keras/resnets_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byTuRGCMYuFW"
      },
      "source": [
        "#resnets_utils.py\n",
        "\n",
        "def load_dataset(): \n",
        "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "    \n",
        "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "    \n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
        "    mini_batch_size - size of the mini-batches, integer\n",
        "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
        "    \n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "    m = X.shape[0]                  # number of training examples\n",
        "    mini_batches = []\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[permutation,:,:,:]\n",
        "    shuffled_Y = Y[permutation,:]\n",
        "    \n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
        "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
        "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    return mini_batches\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)].T\n",
        "    return Y\n",
        "    \n",
        "def forward_propagation_for_predict(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
        "                  the shapes are given in initialize_parameters\n",
        "    Returns:\n",
        "    Z3 -- the output of the last LINEAR unit\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve the parameters from the dictionary \"parameters\" \n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    W3 = parameters['W3']\n",
        "    b3 = parameters['b3'] \n",
        "                                                           # Numpy Equivalents:\n",
        "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
        "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
        "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
        "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
        "    \n",
        "    return Z3\n",
        "    \n",
        "def predict(X, parameters):\n",
        "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
        "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
        "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
        "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
        "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
        "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
        "    \n",
        "    params = {\"W1\": W1,\n",
        "              \"b1\": b1,\n",
        "              \"W2\": W2,\n",
        "              \"b2\": b2,\n",
        "              \"W3\": W3,\n",
        "              \"b3\": b3}\n",
        "    \n",
        "    x = tf.placeholder(\"float\", [12288, 1])\n",
        "    \n",
        "    z3 = forward_propagation_for_predict(x, params)\n",
        "    p = tf.argmax(z3)\n",
        "    \n",
        "    sess = tf.Session()\n",
        "    prediction = sess.run(p, feed_dict = {x: X})\n",
        "         \n",
        "    return prediction "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R468u2175m-"
      },
      "source": [
        "Extracting Files from \"trainPart1.zip\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf2Xcu169XHv",
        "outputId": "3a9eb475-c931-4343-f025-70a089826676"
      },
      "source": [
        "file_name = \"trainPart1.zip\"\n",
        "\n",
        "with ZipFile(file_name, 'r') as zip :\n",
        "  zip.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzHextMC9BqG"
      },
      "source": [
        "DATADIRECTORY = \"/content/train\"\n",
        "CATEGORIES = [\"Sample001\", \"Sample002\", \"Sample003\", \"Sample004\", \"Sample005\", \"Sample006\", \"Sample007\", \"Sample008\", \"Sample009\", \"Sample010\",\n",
        " \"Sample011\", \"Sample012\", \"Sample013\", \"Sample014\", \"Sample015\", \"Sample016\", \"Sample017\", \"Sample018\", \"Sample019\", \"Sample020\",\n",
        " \"Sample021\", \"Sample022\", \"Sample023\", \"Sample024\", \"Sample025\", \"Sample036\", \"Sample027\", \"Sample028\", \"Sample029\", \"Sample030\",\n",
        " \"Sample031\", \"Sample032\", \"Sample033\", \"Sample034\", \"Sample035\", \"Sample036\", \"Sample037\", \"Sample038\", \"Sample039\", \"Sample040\",\n",
        " \"Sample041\", \"Sample042\", \"Sample043\", \"Sample044\", \"Sample045\", \"Sample046\", \"Sample047\", \"Sample048\", \"Sample049\", \"Sample050\",\n",
        " \"Sample051\", \"Sample052\", \"Sample053\", \"Sample054\", \"Sample055\", \"Sample056\", \"Sample057\", \"Sample058\", \"Sample059\", \"Sample060\",\n",
        " \"Sample061\", \"Sample062\"]\n",
        "CATEGORIES = [\"Sample001\", \"Sample002\", \"Sample003\", \"Sample004\", \"Sample005\", \"Sample006\", \"Sample007\", \"Sample008\", \"Sample009\", \"Sample010\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi9mPzuTYEyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d11b4d-19e7-4426-9a33-7fe1f66a7e1f"
      },
      "source": [
        "classSize = len(CATEGORIES)\n",
        "print(classSize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb2kBuOIJimp"
      },
      "source": [
        "IMG_SIZE = 28\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnOaPz7bJoFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa40a6de-76a0-47b6-fc21-400cc5be6d5f"
      },
      "source": [
        "trainingData = []\n",
        "\n",
        "def createTrainingData():\n",
        "    for category in CATEGORIES:\n",
        "\n",
        "        path = os.path.join(DATADIRECTORY,category)\n",
        "        classNum = CATEGORIES.index(category)  # get the classification\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):\n",
        "            try:\n",
        "                imgArray = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
        "                newArray = cv2.resize(imgArray, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                trainingData.append([newArray, classNum])  # add this to our trainingData\n",
        "            except Exception as e:  # keeps output cleans\n",
        "                pass\n",
        "\n",
        "createTrainingData()\n",
        "print(len(trainingData))\n",
        "\n",
        "random.shuffle(trainingData)\n",
        "# we are shuffling the data so that classifier does not \n",
        "# look at same type of images for a long time\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for features, label in trainingData :\n",
        "  X.append(features)\n",
        "  y.append(label)\n",
        "\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:00<00:00, 94.11it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 92.14it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 93.36it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 88.36it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 93.85it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 91.61it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 92.41it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 93.29it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 92.83it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 93.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOeQWnfkiJ0L"
      },
      "source": [
        "**SMALL RESIDUAL NETWORK**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APCSUjCSR-qA"
      },
      "source": [
        "# Normalize image vectors\n",
        "X = X/255.0\n",
        "\n",
        "model = Sequential()\n",
        "# conv layer\n",
        "model.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(256, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # converts our 3D feature vectors to 1D feature vectors\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(classSize))  # our output layer. \n",
        "# Softmax for probability distribution\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # what to track"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTyCtbovIaz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f4666e-05d1-41bd-be6f-d409443958cd"
      },
      "source": [
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y = np.array(y)\n",
        "model.fit(X, y, batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "13/13 [==============================] - 1s 17ms/step - loss: 2.3260 - accuracy: 0.1407\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 15ms/step - loss: 2.3005 - accuracy: 0.1326\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 2.3046 - accuracy: 0.0821\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 2.2974 - accuracy: 0.1193\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 2.2866 - accuracy: 0.1249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f098d586250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-xNJKmf7tvn"
      },
      "source": [
        "Following function loads mnist images (both training and test) from .gz files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDUpq3TZQjLP"
      },
      "source": [
        "def load_data() :\n",
        "\n",
        "  def load_mnist_images(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "      data = data.reshape(-1,28,28,1)\n",
        "      \n",
        "      return data/np.float(256)\n",
        "\n",
        "  def load_mnist_labels(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "      \n",
        "      return data\n",
        "\n",
        "  X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "  y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "  X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "  y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOz4fOtiQn_C"
      },
      "source": [
        "X_train, y_train, X_test, y_test = load_data() # loads mnist data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Z4aWMYQvIl",
        "outputId": "6d8dfd80-61b5-492b-ec79-ee4f32469a0b"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2575 - accuracy: 0.9213\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0850 - accuracy: 0.9780\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0644 - accuracy: 0.9840\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0550 - accuracy: 0.9865\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0469 - accuracy: 0.9884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f09877a4f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tburI58XQ2Ba",
        "outputId": "1ce825d6-2654-4af6-fae6-d70b9313148a"
      },
      "source": [
        "predictions = model.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0438 - accuracy: 0.9876\n",
            "Loss = 0.04376700893044472\n",
            "Test Accuracy = 0.9876000285148621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLfV_yk4TEFR"
      },
      "source": [
        "Test accuracy is pretty good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7M4bu4qIYzF"
      },
      "source": [
        "Above model give results for pretrained model on trainPart1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlHWqY9THcWN"
      },
      "source": [
        "For randomly initialized weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA9g3aw8HYya"
      },
      "source": [
        "# Normalize image vectors\n",
        "X = X/255.0\n",
        "\n",
        "model1 = Sequential()\n",
        "# conv layer\n",
        "model1.add(Conv2D(256, (3, 3), input_shape=X.shape[1:]))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model1.add(Conv2D(256, (3, 3)))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model1.add(Flatten())  # converts our 3D feature vectors to 1D feature vectors\n",
        "\n",
        "model1.add(Dense(64))\n",
        "model1.add(Activation('relu'))\n",
        "\n",
        "model1.add(Dense(128))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "\n",
        "model1.add(Dense(128))\n",
        "model1.add(Activation('relu'))\n",
        "model1.add(Dropout(0.5))\n",
        "\n",
        "model1.add(Dense(classSize))  # our output layer. \n",
        "# Softmax for probability distribution\n",
        "model1.add(Activation('softmax'))\n",
        "\n",
        "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # what to track"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsSYpPeVHo-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07221bfc-b0d5-42e9-bff2-01d8689fffc1"
      },
      "source": [
        "model1.fit(X_train, y_train, batch_size=32, epochs=5)\n",
        "predictions = model1.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 27s 14ms/step - loss: 0.5304 - accuracy: 0.8202\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0783 - accuracy: 0.9789\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0573 - accuracy: 0.9868\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0446 - accuracy: 0.9897\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0348 - accuracy: 0.9913\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0620 - accuracy: 0.9856\n",
            "Loss = 0.0620059035718441\n",
            "Test Accuracy = 0.9855999946594238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhl0F9bTgb3I"
      },
      "source": [
        "**RESNET50**\n",
        "\n",
        "Implementation inspiration from\n",
        "- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - [Deep Residual Learning for Image Recognition (2015)](https://arxiv.org/abs/1512.03385)\n",
        "- Francois Chollet's GitHub repository: https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKy4KiJ2gfXG"
      },
      "source": [
        "# GRADED FUNCTION: identity_block\n",
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # Skips over 3 hidden layers\n",
        "\n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "        \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X,X_shortcut])\n",
        "    # pass it through a RELU activation\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z-aHIVMhz38"
      },
      "source": [
        "# GRADED FUNCTION: convolutional_block\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
        "\n",
        "    # defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Extract Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value as shortcut\n",
        "    X_shortcut = X\n",
        "\n",
        "    # MAIN PATH #\n",
        "\n",
        "    # First component of main path \n",
        "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(F2, (f, f), strides = (1,1), name = conv_name_base + '2b', padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # SHORTCUT PATH #\n",
        "    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Add shortcut value to main path\n",
        "    X = Add()([X, X_shortcut])\n",
        "    # pass through RELU activation\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeZGeuXbh6yX"
      },
      "source": [
        "# GRADED FUNCTION: ResNet50\n",
        "# returns resnet model\n",
        "def ResNet50(input_shape, classes):\n",
        "    \"\"\"\n",
        "    Implementation of the popular ResNet50 the following architecture:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    # Zero-Padding, pads the input with pad(3,3)\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = AveragePooling2D((2, 2), padding='same')(X)\n",
        "    \n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbvrjB8aiR2F"
      },
      "source": [
        "model = ResNet50(input_shape = (IMG_SIZE, IMG_SIZE, 1), classes = classSize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzYSxKstijwg"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi04Iu8wimtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7c656a-9e7e-48d2-949a-c07d01e4fc52"
      },
      "source": [
        "# Normalize image vectors\n",
        "X = X/255.\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "y = convert_to_one_hot(y, classSize).T\n",
        "\n",
        "print (\"number of training examples = \" + str(X.shape[0]))\n",
        "print (\"X shape: \" + str(X.shape))\n",
        "print (\"y shape: \" + str(y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 400\n",
            "X shape: (400, 28, 28, 1)\n",
            "y shape: (400, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TjULdxKjEQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611e4306-bcf0-4a15-cb48-d3f51801596d"
      },
      "source": [
        "# model.fit(X, y, epochs = 6, batch_size = 32)\n",
        "model.fit(X, y, batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "13/13 [==============================] - 7s 96ms/step - loss: 4.9800 - accuracy: 0.0954\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 3.4542 - accuracy: 0.2532\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 2.5368 - accuracy: 0.3736\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 1.6258 - accuracy: 0.5200\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 1s 87ms/step - loss: 1.3642 - accuracy: 0.5590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f098092c910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl-7yJBfjLdc"
      },
      "source": [
        "def load_data() :\n",
        "\n",
        "  def load_mnist_images(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "      data = data.reshape(-1,28,28,1)\n",
        "      \n",
        "      return data/np.float(256)\n",
        "\n",
        "  def load_mnist_labels(filename) :\n",
        "\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "      data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "      \n",
        "      return data\n",
        "\n",
        "  X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
        "  y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
        "  X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
        "  y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "  return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPQXqxlE5bMc"
      },
      "source": [
        "X_train, y_train, X_test, y_test = load_data() # loads mnist data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv5Nia3G5eTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7bac67e-3459-4d48-c5ed-4e2fcc21ca85"
      },
      "source": [
        "# Normalize image vectors\n",
        "X_train = X_train/255.\n",
        "X_test = X_test/255.\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "y_train = convert_to_one_hot(y_train, classSize).T\n",
        "y_test = convert_to_one_hot(y_test, classSize).T\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
        "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"y_train shape: \" + str(y_train.shape))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"y_test shape: \" + str(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples = 60000\n",
            "number of test examples = 10000\n",
            "X_train shape: (60000, 28, 28, 1)\n",
            "y_train shape: (60000, 10)\n",
            "X_test shape: (10000, 28, 28, 1)\n",
            "y_test shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK4zqSFJ6RKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6346e3f-9944-4f31-843a-875c86e61401"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 172s 90ms/step - loss: 0.3048 - accuracy: 0.9197\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.2759 - accuracy: 0.9458\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.1925 - accuracy: 0.9546\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.1191 - accuracy: 0.9707\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 168s 90ms/step - loss: 0.1770 - accuracy: 0.9586\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f097fd32790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LJ8NiF26jw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6037c8-c620-4681-811f-c021bdf3a6b6"
      },
      "source": [
        "predictions = model.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 8s 24ms/step - loss: 0.4471 - accuracy: 0.9197\n",
            "Loss = 0.44712918996810913\n",
            "Test Accuracy = 0.919700026512146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Xkh8vWhdER"
      },
      "source": [
        "Sequential model outperformed resnet50 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6utFR6E9i4LQ"
      },
      "source": [
        "Now let us train randomly initialized network (without training on trainPart1.zip files)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoDfLlESh50_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8525dd7-1910-4441-f523-4f5ea8cdc741"
      },
      "source": [
        "model = ResNet50(input_shape = (IMG_SIZE, IMG_SIZE, 1), classes = classSize)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=5)\n",
        "\n",
        "predictions = model.evaluate(X_test, y_test)\n",
        "print (\"Loss = \" + str(predictions[0]))\n",
        "print (\"Test Accuracy = \" + str(predictions[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 173s 89ms/step - loss: 0.7068 - accuracy: 0.8400\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 168s 89ms/step - loss: 0.2692 - accuracy: 0.9415\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 168s 89ms/step - loss: 0.1152 - accuracy: 0.9721\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 168s 89ms/step - loss: 0.1427 - accuracy: 0.9604\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 167s 89ms/step - loss: 0.1715 - accuracy: 0.9621\n",
            "313/313 [==============================] - 8s 23ms/step - loss: 1.1492 - accuracy: 0.5786\n",
            "Loss = 1.1491684913635254\n",
            "Test Accuracy = 0.5785999894142151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFtzrVjQ7R6O"
      },
      "source": [
        "For sequential model test accuracies were pretty much same for both pretrained and randomly initialized models (and were pretty high compared to RESNET50) but it not case with RESNET50 test accuracies vary significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7uEhl_QjSyn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}